{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "¿Por qué el modelo de Naive Bayes se le considera “naive”?\n",
    "El modelo asume que todas las características utilizadas para describir una instancia son independientes entre sí, dado el valor de la clase a la que pertenece la instancia. Aunque esta suposición es simplificadora y rara vez es verdadera en la práctica, el modelo Naive Bayes sigue siendo efectivo en muchos casos y puede ser computacionalmente eficiente.\n",
    "La \"naivete\" proviene de la simplicidad de esta suposición, ya que en situaciones del mundo real, las características suelen estar correlacionadas entre sí. Sin embargo, a pesar de esta ingenuidad, el modelo de Naive Bayes a menudo proporciona resultados sorprendentemente buenos, especialmente en problemas de clasificación de texto y minería de texto, donde puede ser utilizado para tareas como la clasificación de spam o la categorización de documentos.\n",
    "Explique la formulación matemática que se busca optimizar en Support Vector Machine, además responda ¿cómo funciona el truco del Kernel para este modelo? (Lo que se espera de esta pregunta es que puedan explicar en sus propias palabras la fórmula a la que llegamos que debemos optimizar de SVM en clase)\n",
    "La formulación matemática para el SVM se basa en la minimización de la norma del vector de pesos (parámetros del hiperplano), sujeto a ciertas restricciones. Para un problema de clasificación binaria, la formulación típica es:\n",
    "\n",
    "El truco del kernel en SVM es una técnica que permite manejar de manera eficiente conjuntos de datos no lineales. Consiste en transformar el espacio de características original a un espacio de características de mayor dimensión mediante una función kernel. La formulación matemática del truco del kernel se expresa como:\n",
    "\n",
    "\n",
    "Investigue sobre Random Forest y responda\n",
    "¿Qué tipo de ensemble learning es este modelo?\n",
    "¿Cuál es la idea general detrás de Random Forest?\n",
    "¿Por qué se busca baja correlación entre los árboles de Random Forest?\n",
    "Tipo de Ensemble Learning:\n",
    "Random Forest es un modelo de ensemble learning que pertenece a la categoría de \"bagging\" (bootstrap aggregating). El bagging es una técnica que combina múltiples modelos de manera paralela, entrenándolos de forma independiente y luego promediando (en el caso de regresión) o votando (en el caso de clasificación) para obtener la predicción final.\n",
    "\n",
    "Idea General detrás de Random Forest:\n",
    "La idea principal detrás de Random Forest es construir múltiples árboles de decisión durante el entrenamiento y combinar sus resultados para obtener una predicción más robusta y generalizable. Cada árbol de decisión en el bosque se entrena en una submuestra aleatoria del conjunto de datos, y además, durante la construcción de cada árbol, se seleccionan aleatoriamente un subconjunto de características para cada división del nodo. Esto introduce variabilidad y diversidad en los árboles, lo que contribuye a un modelo más resistente al sobreajuste y capaz de manejar datos no lineales o complejos.\n",
    "\n",
    "Baja correlación entre los árboles:\n",
    "La baja correlación entre los árboles en un Random Forest es fundamental para su eficacia. Al construir cada árbol con diferentes subconjuntos de datos y características, se promueve la independencia entre ellos. La baja correlación entre los árboles significa que si un árbol comete un error en ciertas instancias, es menos probable que otros árboles cometan el mismo error en esas instancias. Por lo tanto, la combinación de múltiples árboles con baja correlación tiende a reducir la varianza del modelo general, mejorando así la capacidad de generalización y haciendo que el modelo sea más robusto ante diferentes tipos de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2.4 - Comparación con Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión en el conjunto de entrenamiento: 0.9941573033707866\n",
      "Precisión en el conjunto de prueba: 0.9847259658580413\n",
      "\n",
      "Informe de Clasificación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      0.99       958\n",
      "        spam       0.98      0.91      0.94       155\n",
      "\n",
      "    accuracy                           0.98      1113\n",
      "   macro avg       0.98      0.95      0.97      1113\n",
      "weighted avg       0.98      0.98      0.98      1113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Para realizar el mismo procedimiento pero con una libreria utilizaremos \"sklearn\"\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import os\n",
    "\n",
    "#Se utilizo pandas con la libreria \"os\" para leer el archivo desde el directorio actual \n",
    "df = pd.read_csv(os.path.join(os.getcwd(), \"Bayes\", \"entrenamiento.txt\"),delimiter=\"\\t\", header=None , names=[\"status\",\"message\"])\n",
    "\n",
    "# Se dividio el dataset en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['message'], df['status'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Se vectorizaron de los mensajes\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "# Naive Bayes con sklearn\n",
    "model_sklearn = MultinomialNB()\n",
    "model_sklearn.fit(X_train_vect, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_train = model_sklearn.predict(X_train_vect)\n",
    "y_pred_test = model_sklearn.predict(X_test_vect)\n",
    "\n",
    "#Informe de clasificación\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "report = classification_report(y_test, y_pred_test)\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Precisión en el conjunto de entrenamiento:\", accuracy_train)\n",
    "print(\"Precisión en el conjunto de prueba:\", accuracy_test)\n",
    "print(\"\\nInforme de Clasificación:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ¿Cuál implementación lo hizo mejor? ¿Su implementación o la de la librería?\n",
    "Tiene un cierto parecido pero el accuracy de la libreria fue de 98% y el que no tiene librería es de 99%, por es mejor realizarlo por uno mismo ya que mejoran los resultados\n",
    "##### ¿Por qué cree que se debe esta diferencia?\n",
    "Ya que las formulas son mas exactas al realizar el procedimiento manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7327935222672065\n",
      "ROC-AUC: 0.7327918009996732\n",
      "Precision: 0.7346115035317861\n",
      "Recall: 0.7331319234642497\n",
      "F1 Score: 0.7338709677419355\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Cargar el dataset\n",
    "file_path = \"high_diamond_ranked_10min.csv\"  # Reemplaza con la ruta correcta a tu archivo\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Eliminar la columna 'gameId'\n",
    "data = data.drop(columns=['gameId'])\n",
    "\n",
    "# Separar la variable objetivo y las características\n",
    "X = data.drop('blueWins', axis=1)\n",
    "y = data['blueWins']\n",
    "\n",
    "# Escalar las características\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Dividir el dataset en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo de Regresión Logística\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Imprimir los resultados de las métricas\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"ROC-AUC: {roc_auc}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.3: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "¿Cómo difirieron los grupos creados por ambos modelos?\n",
    "Naturaleza del modelo:\n",
    "\n",
    "SVM: Es un modelo basado en márgenes. Busca encontrar el hiperplano que maximiza la distancia entre las clases en el espacio de características.\n",
    "Árbol de decisión: Es un modelo basado en reglas de decisión. Divide el espacio de características en regiones mediante un conjunto de reglas de decisión basadas en los valores de las características.\n",
    "Complejidad del modelo:\n",
    "\n",
    "SVM: Puede ser efectivo en espacios de alta dimensión y es robusto frente a datos ruidosos, pero puede volverse computacionalmente costoso en conjuntos de datos muy grandes.\n",
    "Árbol de decisión: Puede ser más interpretable y fácil de entender. Sin embargo, es propenso al sobreajuste, especialmente en conjuntos de datos complejos.\n",
    "Flexibilidad y no linealidad:\n",
    "\n",
    "SVM: Puede manejar problemas no lineales mediante el uso de funciones de kernel para mapear los datos a un espacio de características de mayor dimensión.\n",
    "Árbol de decisión: Es capaz de manejar relaciones no lineales en los datos de forma natural sin requerir transformaciones adicionales.\n",
    "Interpretabilidad:\n",
    "\n",
    "SVM: Aunque puede proporcionar buenos resultados de clasificación, la interpretación del modelo puede ser más desafiante debido a su naturaleza basada en márgenes y funciones de kernel.\n",
    "Árbol de decisión: Es más interpretable, ya que las reglas de decisión se pueden visualizar y entender fácilmente. Cada nodo representa una decisión basada en una característica específica.\n",
    "\n",
    "¿Cuál de los modelos fue más rápido?\n",
    "Para nuestros resultados el SVM fue el más rápido de los dos modelos.\n",
    "¿Qué modelo usarían?\n",
    "Nosotros nos quedamos con el modelo SVM para futuros usos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
